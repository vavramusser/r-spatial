{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72003a39-c3a2-4e91-831a-c504e260f92a",
   "metadata": {},
   "source": [
    "# IPUMS NHGIS Data Extraction Using ipumsr\n",
    "### by [Kate Vavra-Musser](https://vavramusser.github.io) for the [R Spatial Notebook Series](https://vavramusser.github.io/r-spatial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946f837-bf45-4140-a611-445463e263ad",
   "metadata": {},
   "source": [
    "This notebook builds on the the workflow introduced in the **[Introduction to the IPUMS API for R Users](https://tech.popdata.org/ipumsr/articles/ipums-api.html)** article on the IPUMS website.  As the author of the R Spatial Notebook series, I recognize the IPUMS article as a significant inspiration and source of information for this notebook.\n",
    "\n",
    "## Introduction\n",
    "The [IPUMS NHGIS](https://www.nhgis.org) database offers harmonized summary data and geographic boundary files from U.S. censuses and surveys, providing a resource for spatial analysis of demographic, social, and economic trends. It enables users to access aggregated data at various geographic levels, such as states, counties, and census tracts, facilitating the exploration of population dynamics and regional patterns over time. Through harmonization, IPUMS NHGIS ensures that data can be seamlessly compared across years, despite changes in geographic boundaries, variable definitions, and survey methodologies.\n",
    "\n",
    "**From the [IPUMS NHGIS Webpage](https://www.nhgis.org):** The National Historical Geographic Information System (NHGIS) provides easy access to summary tables and time series of population, housing, agriculture, and economic data, along with GIS-compatible mapping files, for years from 1790 through the present and for all levels of U.S. census geography, including states, counties, tracts, and blocks.\n",
    "\n",
    "#### Data Included in the IPUMS NHGIS Repository (Available Using the IPUMS API)\n",
    "* [Decennial Census](https://www.census.gov/programs-surveys/decennial-census.html) data from 1790 to present\n",
    "* [American Community Survey (ACS)](https://www.census.gov/programs-surveys/acs/about.html) data from 2009 to present\n",
    "\n",
    "#### Additional NHGIS Data Available Using the [NHGIS Online Data Finder](https://data2.nhgis.org/main)\n",
    "* [Census of Agriculture](https://www.nass.usda.gov/AgCensus) data from 1850 to 1959\n",
    "* [County Business Patterns (CBP)](https://www.census.gov/programs-surveys/cbp.html) data from 1970 to 2002\n",
    "* Census of Religious Bodies data from 1906, 1916, 1926, and 1936\n",
    "* Marriage and Divorce data from 1867 to 2010\n",
    "* Natality and Mortality Data from 1915 to 2007\n",
    "* 1952 Survey of Churches and Church Membership\n",
    "* 1920-1936 FDIC Bank Deposit Data\n",
    "* 1925 Special Census of Detroit\n",
    "* 1937 Census of Unemployment\n",
    "\n",
    "### About the Decennial Census\n",
    "The United States **[Decennial Census](https://www.census.gov/programs-surveys/decennial-census.html)** is a population and housing count conducted by the [U.S. Census Bureau](https://www.census.gov) every ten years. The Census aims to count every person living in the United States and its territories, collecting basic demographic information such as age, sex, race, ethnicity, and household relationships. This data is used primarily to allocate seats in the U.S. House of Representatives, redraw congressional and legislative districts, and distribute federal funding to states and local communities.\n",
    "\n",
    "The Decennial Census is designed to provide a comprehensive snapshot of the nation's population and housing characteristics at a specific point in time. The data collected is crucial for planning and policy-making, as well as for guiding resource allocation for schools, hospitals, infrastructure projects, and emergency response services. Unlike the more detailed American Community Survey (ACS), which samples a subset of the population annually, the Decennial Census seeks to account for the entire population in one effort, making it a critical tool for understanding population dynamics over time.\n",
    "\n",
    "### About the American Community Survey (ACS)\n",
    "The **[American Community Survey (ACS)](https://www.census.gov/programs-surveys/acs/about.html)** is an annual survey conducted by the [U.S. Census Bureau](https://www.census.gov) that collects information on a subset of the U.S. population.  The ACS collects data on a variety of topics, including income, poverty, education, marital status, health insurance coverage, disability, occupancy, costs, tenure, and units by type.  It is a more in-depth supplement to the Decennial U.S. Census and in 2005 replaced the long-form version of the Decennial Census survey which was previously conducted every ten years.  Each year the ACS samples over 3.5 million housing units across the United States with a new sample of about 250,000 addresses drawn each month.\n",
    "\n",
    "ACS data is available as single-year datasets as well as three- and five-year summaries of the data.  While single-year data provide a snapshot of conditions in a specific year, the three- and five-year summaries offer more stable estimates by averaging data over time, making them less susceptible to anomalies and more useful for analyzing smaller geographic areas.\n",
    "\n",
    "### Notebook Goals\n",
    "This notebook introduces the process of extracting [IPUMS NHGIS](https://www.nhgis.org) data using the [IPUMS API](https://developer.ipums.org/docs/v2/apiprogram) via the [ipumsr R package](https://cran.r-project.org/web/packages/ipumsr/index.html). Users will learn how to define, submit, and download an IPUMS NHGIS data extract, specifying desired variables, time periods, and geographic units for analysis. By the end of this notebook, users will have the skills to efficiently acquire customized IPUMS NHGIS datasets and prepare them for spatial and statistical workflows.\n",
    "\n",
    "### ✨ Prerequisites ✨\n",
    "* Complete [Introduction to IPUMS and the IPUMS API](https://platform.i-guide.io/notebooks/82d3b176-e4e6-4307-8186-318a3fe6c81a)\n",
    "* Set Up Your [IPUMS Account and API Key](https://account.ipums.org/api_keys)\n",
    "* Complete [Introduction to sf: Reading, Writing, and Inspecting Vector Data](https://platform.i-guide.io/notebooks/9968babe-22e4-4c3d-98e2-d8b45e9672cd)\n",
    "\n",
    "### Notebook Overview\n",
    "1. Setup\n",
    "2. IPUMS NHGIS Time-Series Data Metadata Exploration\n",
    "3. IPUMS NHGIS Geography Shapefile Metadata Exploration\n",
    "4. IPUMS NHGIS Time-Series Data and Geography Shapefile Extraction Specification and Submission\n",
    "5. Subset and Merge the Time-Series and Geography Data Extractions("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b0d57-13c7-49d7-8511-85fb37e1b296",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "This section will guide you through the process of installing essential packages and setting your IPUMS API key.\n",
    "\n",
    "#### Required Packages\n",
    "\n",
    "[**dplyr**](https://cran.r-project.org/web/packages/dplyr/index.html) A Grammar of Data Manipulation. This notebook uses the the following functions from *dplyr*.\n",
    "\n",
    "* [*filter*](https://rdrr.io/cran/dplyr/man/filter.html) · keep rows that match a condition\n",
    "* [*select*](https://rdrr.io/cran/dplyr/man/select.html) · keep or drop columns using their names and types\n",
    "* [*rename*](https://rdrr.io/cran/dplyr/man/rename.html) · rename columns\n",
    "* This notebook also uses [*%>%*](https://magrittr.tidyverse.org/reference/pipe.html), referred to as the *pipe* operator.  The *pip* operator is used to pass the output from one function directly into the next function for the purpose of creating streamlined workflows and is a commonly used component of the [*tidyverse*](https://www.tidyverse.org).\n",
    "\n",
    "[**geojsonio**](https://cran.r-project.org/web/packages/geojsonio/index.html) · Convert Data from and to *[GeoJSON](https://geojson.org)* or *[TopoJSON](https://github.com/topojson/topojson)*.\n",
    "\n",
    "[**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html) An R Interface for Downloading, Reading, and Handling IPUMS Data.  This notebook uses the the following functions from *ipumsr*.\n",
    "\n",
    "* [*define_extract_nhgis*](https://rdrr.io/cran/ipumsr/man/define_extract_nhgis.html) · define an IPUMS NHGIS extract request\n",
    "* [*download_extract*](https://rdrr.io/cran/ipumsr/man/download_extract.html) · download a completed IPUMS data extract\n",
    "* [*get_metadata_nhgis*](https://rdrr.io/cran/ipumsr/man/get_metadata_nhgis.html) · list available data sources from IPUMS NHGIS\n",
    "* [*read_ipums_sf*](https://rdrr.io/cran/ipumsr/man/read_ipums_sf.html) · read spatial data from an IPUMS extract\n",
    "* [*read_nhgis*](https://rdrr.io/cran/ipumsr/man/read_nhgis.html) · read tabular data from an NHGIS extract\n",
    "* [*set_ipums_api_key*](https://rdrr.io/cran/ipumsr/man/set_ipums_api_key.html) · set your IPUMS API key\n",
    "* [*submit_extract*](https://rdrr.io/cran/ipumsr/man/submit_extract.html) · submit an extract request via the IPUMS API\n",
    "* *tst_spec* · create a *tst_spec* object containing a time series table specification\n",
    "* [*wait_for_extract*](https://rdrr.io/cran/ipumsr/man/wait_for_extract.html) · wait for an extract to finish processing\n",
    "\n",
    "[**purrr**](https://cran.r-project.org/web/packages/purrr/index.html) A complete and consistent functional programming toolkit for R. This notebook uses the the following functions from *purrr*.\n",
    "\n",
    "* [*map()*](https://rdrr.io/cran/purrr/man/map.html) and [*map_dfr()*](https://rdrr.io/cran/purrr/man/map_dfr.html) · apply a function to each element of a vector\n",
    "\n",
    "[**sf**](https://cran.r-project.org/web/packages/sf/index.html) Support for simple features, a standardized way to encode spatial vector data. Binds to 'GDAL' for reading and writing data, to 'GEOS' for geometrical operations, and to 'PROJ' for projection conversions and datum transformations. Uses by default the 's2' package for spherical geometry operations on ellipsoidal (long/lat) coordinates.  This notebook uses the following functions from *sf*.\n",
    "\n",
    "* [*st_write*](https://rdrr.io/cran/sf/man/st_write.html) · Write simple features object to file or database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a2de1-61ff-4e6e-a732-c5324df37b34",
   "metadata": {},
   "source": [
    "### 1a. Install and Load Required Packages\n",
    "If you have not already installed the required packages, uncomment and run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b426c39-2d8d-4562-a284-d063f6f918e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"dplyr\", \"geojsonio\", \"ipumsr\", \"purr\", \"sf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd438188-d2c1-4a3d-8cec-55408980826e",
   "metadata": {},
   "source": [
    "Load the packages into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65683ba8-4ee0-447f-8cd3-36e76f469f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(geojsonio)\n",
    "library(ipumsr)\n",
    "library(purrr)\n",
    "library(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b81dee-1b3e-4ac2-9649-a38f28f37c45",
   "metadata": {},
   "source": [
    "### 1b. Set Your IPUMS API Key\n",
    "\n",
    "Store your [IPUMS API key](https://account.ipums.org/api_keys) in your environment using the following code.\n",
    "\n",
    "Refer to [Chapter 1.1 Introduction to IPUMS and the IPUMS API](https://platform.i-guide.io/notebooks/82d3b176-e4e6-4307-8186-318a3fe6c81a) for instructions on setting up your IPUMS account and API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf136374-a2ee-4a3f-a633-fe33e23c48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipumps_api_key = readline(\"Please enter your IPUMS API key: \")\n",
    "set_ipums_api_key(ipumps_api_key, save = T, overwrite = T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c68d863-f0c8-4394-b289-7d021433e46c",
   "metadata": {},
   "source": [
    "## 2. NHGIS Time-Series Data Metadata Exploration\n",
    "\n",
    "Before submitting an IPUMS data extraction request, it’s essential to ensure the parameters of the extraction definition are set up correctly.  The extraction definition specifies the sample, variables, geographic levels, and other options.\n",
    "\n",
    "If this is your first time using the IPUMS API in R, or if you are setting up a new data extract for a new project, it is a good idea to start by exploring the available data which can be done using the *ipumsr* package.\n",
    "\n",
    "Because we are extracting data from the IPUMS NHGIS repository, we will carry out the process of setting up our extraction in two phases.  First we will set up the extraction paramaters for the tabular component of our data and second we will set up the extraction parameters for the spatial component of our data.\n",
    "\n",
    "**★ Pro Tip:** The NHGIS extraction setup process in R is significantly different from the extraction process used for other IPUMS data repositories.  If this is your first time setting up an NHGIS data extraction in R, please be sure to carefully follow all steps in this notebook, even if you have previously used R to extract data from other IPUMS repositories.\n",
    "\n",
    "### 2a. Review the List of Time-Series Datasets\n",
    "\n",
    "First, let's take a look at the entire database of time-series datasets available from the [IPUMS NHGIS data repository](https://www.nhgis.org).  The NHGIS data available for direct extraction using the IPUMS API include the [Decennial Census](https://www.census.gov/programs-surveys/decennial-census.html) and [American Communinty Survey (ACS)](https://www.census.gov/programs-surveys/acs).\n",
    "\n",
    "**★ Pro Tip:** The [NHGIS Online Data Finder](https://data2.nhgis.org/main) provides access to other NHGIS data sources not available via API including the [Census of Agriculture](https://www.nass.usda.gov/AgCensus), [County Business Patterns (CBP)](https://www.census.gov/programs-surveys/cbp.html), and other historic data samples.\n",
    "\n",
    "For this step, we will use the [*get_metadata_nhgis*](https://rdrr.io/cran/ipumsr/man/get_metadata_nhgis.html) function from the [**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html) package.  This function will return a database of all population datasets witin the IPUMS NHGIS data repository which are available to be downloaded using the IPUMS API.  This code stores the metadata from all available samples in the IPUMS USA repository to the object *metadata_datts* and prints the first 10 lines of the metadata database so we can preview the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2d502-001d-474e-a02e-a5529c64bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of time-series dataset metadata\n",
    "metadata_datts <- get_metadata_nhgis(\"time_series_tables\") %>% print(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b9822-fbc1-40e7-9a65-1149ae59e936",
   "metadata": {},
   "source": [
    "From this preview, we can see that the IPUMS USA metada table has a **name**, corresponding to a sample identification code, a **description**, providing a short description or label for each sample, **geographic_integration** providing information on the type of spatial data which can be linked to it, **sequence**, and three colums (**time_series**, **years**, and **geo_levels**) which are each represented as [*tibbles*](https://tibble.tidyverse.org).  We will need to select a sample and make note of its sample identification code (**name**) which we will use when defining our data extraction.\n",
    "\n",
    "**★ Pro Tip:** If you are working in Jupyter Notebooks your view of the columns in the data table may be truncated.  Take a look at the bottom of the table view to see the number of rows and variables (columns) which are not included in the preview.\n",
    "\n",
    "A [*tibble*](https://tibble.tidyverse.org) can be thought of as a version of a data.frame that includes additional functionality and metadata visibility.  It is also more compatible with the [*tidyverse*](https://www.tidyverse.org) packages, including the [*dplyr*](https://cran.r-project.org/web/packages/dplyr/index.html) package we use in this notebook.  The information in the *tibble* colums is not visualized in this high-level view of the data but you can imagine that for each *\\<tibble>* entry there is another table of information containing additional details on the available data.  We will zoom in deeper in the following steps and you will be able to view the data contained within these tibbles.\n",
    "\n",
    "From this view we can also see that there are 389 samples available in from IPUMS NHGIS.  If we consider that each *\\<tibble>* entry represents an additional table with multiple options for *time_series*, *years*, and *geog_levels* for each datasets, the wealth of data becomes overwhelming.\n",
    "\n",
    "At first glance, it might be difficult to understand what data is contained in each sample, especially if you are not used to working with US Census and ACS data.  Refer to the [Overview of NHGIS Datasets](https://www.nhgis.org/overview-nhgis-datasets) page on the IPUMS NHGIS website for a list of all IPUMS NHGIS samples and more detailed information on each sample and the [List of NHGIS Time Series Tables](https://assets.nhgis.org/NHGIS_Time_Series_Tables.pdf) for a list time-series sample identification codes and additional information.\n",
    "\n",
    "### 2b. Filter Metadata by Criteria\n",
    "\n",
    "If you already know which sample you want to use you could explore the samples list until you found the appropriate sample identification code (**name**).  Alternately, you could use the [*filter*](https://rdrr.io/cran/dplyr/man/filter.html) function from the [**dplyr**](https://cran.r-project.org/web/packages/dplyr/index.html) package in conjunction with the[*str_detect*](https://stringr.tidyverse.org/reference/str_detect.html) function from the [**stringr**](https://cran.r-project.org/web/packages/stringr/index.html) package to filter the list of samples down to the subset which may be relevant for your project.\n",
    "\n",
    "For this exercise, we will filter the list of sample metadata *metadata_datts* to only samples which have the phrase *total population* in their descriptions.  The code below uses the [*filter*](https://rdrr.io/cran/dplyr/man/filter.html) function in conjunction with the [*select*](https://rdrr.io/cran/dplyr/man/select.html), *as.data.frame*, and *print* functions to filter the metadata table to only the datasets which meet our criteria, only print the *name* and *description* for the resulting datasets, and print the results as a *data.frame* rather than the *tibble* format which the *metadata_datts* object is stored in.  The *as.data.frame* and *print* steps is purely to create a clean and easy-to-read result of the filter process and shows us only the most basic information we need to be able proceed to our next step.  The complete table of results from the filtering step, including all metadata, is stored in the *metadat_datts_filter* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ccea9-a365-458d-8600-dbc7edc8212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_datts_filter <- metadata_datts %>%\n",
    "    filter(grepl(\"total population\", description, ignore.case = T)) %>%\n",
    "    select(name, description) %>%\n",
    "    as.data.frame() %>%\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95966ce-70ff-4d03-bd51-029805b14816",
   "metadata": {},
   "source": [
    "The filtering process has returned four relevant samples which have total population information.  Next we will dig deeper into the metadata for this selection of datasets datasets to determine which of the datasets includes information on the time range and geographies we are interested in.\n",
    "\n",
    "### 2c. Identify Available Years and Geographic Levels\n",
    "\n",
    "Next will display the available years and geographic levels for the potential datasets.  This step reveals the information which is stored as nested *tibbles* for each sample and not visible in the high-level metadata table.  We will use the [*get_metadata_nhgis*](https://rdrr.io/cran/ipumsr/man/get_metadata_nhgis.html) command from the [**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html) to view metadata details for a specific NHGIS time-series table by passing the idenfitication code (*name*) to the function.\n",
    "\n",
    "The following example shows the metadata for the *CL8* time-series table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998802a-203f-46bb-8b95-6af790c8ba93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_metadata_nhgis(time_series_table = \"CL8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e13a9d-cc92-456d-ae13-fa6a6b65720f",
   "metadata": {},
   "source": [
    "The metadata view shows that the *CL8* time-series table has the following characteristics:\n",
    "\n",
    "* the table includes data on **total population**\n",
    "* it is **standardized to 2010 geographies**\n",
    "* the data is available for the **years 1990, 2000, 2010, and 2020**\n",
    "* and available at **multiple geography levels** including state, county, tract, block group, county subdivision, place, congressional district, CBSA, urban area, and ZIP code.\n",
    "\n",
    "This view shows us all the information included in the **time_series**, **years**, and **geog_levels** *tibbles* which were obscured in the high-level view in step 2a.\n",
    "\n",
    "We could repeat this process for each of the four time-series tables which were returned in our data filtering proces, but to save us some time, the code below takes the **name**, **description**, **years**, and **geograpic levels** information for each of the tables in our filtering results and presents the metadata in a simple reference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be7982-9fde-45b2-830f-21536d5708bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata for each time-series table\n",
    "metadata_list <- map(metadata_datts_filter$name, ~ get_metadata_nhgis(time_series_table = .x))\n",
    "\n",
    "# combine into a data frame with the necessary columns\n",
    "metadata_combined <- map_dfr(metadata_list, function(metadata) {\n",
    "  data.frame(\n",
    "    name = metadata$name,\n",
    "    description = metadata$description,\n",
    "    # extract only the \"description\" column from the nested tibbles in \"years\" and \"geog_levels\"\n",
    "    years = paste(metadata$years$description, collapse = \", \"),\n",
    "    geog_levels = paste(metadata$geog_levels$name, collapse = \", \")\n",
    "  )\n",
    "})\n",
    "\n",
    "# print the final data frame\n",
    "metadata_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf2630-34dd-4619-8c1a-304008143778",
   "metadata": {},
   "source": [
    "Looking at these results, we can easily see the available years and geographies for each of the time-series tables we identified in our earlier filtering process.\n",
    "\n",
    "**★ Pro Tip:** Note that the lists of year ranges include both single data (e.g. \"2000\") from the [Decennial Census](https://www.census.gov/programs-surveys/decennial-census.html) and year ranges (e.g. \"2008-2012\") corresponding to  five-year population estimates from the [American Community Survey (ACS)](https://www.census.gov/programs-surveys/acs).  For example, the *B78* table includes total population counts for 1980, 1990, 2000, 2010, and 2020 from the Decennial Census and five-year population estimates from the ACS for 2007-2011, 2008-2012, 2009-2013, 2010-2014, 2011-2015, 2012-2016, 2013-2017, 2014-2018, 2015-2019, 2020, 2016-2020, 2017-2021, 2018-2022, and 2019-2023.  Refer to the [List of NHGIS Time Series Tables](https://assets.nhgis.org/NHGIS_Time_Series_Tables.pdf) on the IPUMS NHGIS website for additional information on data sources for each of the time-series tables in the NHGIS repository.\n",
    "\n",
    "**★ Pro Tip:** For this exercise we focused on time-series datasets with informaiton on total population.  However, the IPUMS NHGIS repository includes data on many more topics and many other sources besides the Decennial Census and ACS.  Refer to the [Overview of NHGIS Datasets](https://www.nhgis.org/overview-nhgis-datasets) and the [List of NHGIS Time Series Tables](https://assets.nhgis.org/NHGIS_Time_Series_Tables.pdf) on the IPUMS NHGIS website for additional information on all available datasets.\n",
    "\n",
    "### 2d. Select a Time-Series Dataset\n",
    "\n",
    "Once we have decided on a specific dataset, we will save the table's code for use in our data extration later on.\n",
    "\n",
    "For this example, we will select **table CL8** which includes total population information from the Decennial Census for 1990, 2000, 2010, and 2020 harmonized to 2010 geographies.  You could simply make a mental note of the identification code *CL8* but to make things easier for us we will save the table's code to the *selection_datts* object and use this for our data extraction later on.\n",
    "\n",
    "**★ Pro Tip:** You can select multiple datasets by passing a list of identificaiton codes in the data extraction step (e.g. *c(\"CL8\", \"A00\")*).  However, if you choose to select multiple datasets, be mindful of the differences in available years and geographies for the datasets in your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb219d2-fa44-4a90-844d-6b466cb4ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_datts <- \"CL8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51bfe5-2dc5-4d22-9361-6c4532d33e7c",
   "metadata": {},
   "source": [
    "Now that we have selected our dataset, let's review the table's complete metadata details again.  It's a good idea to verfity that we have selected the correct dataset and that our selection meets the needs of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9007030-ac86-44b1-bd5e-efa9c569147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metadata_nhgis(time_series_table = selection_datts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a95cdd-2313-498c-9d0c-d2438cced412",
   "metadata": {},
   "source": [
    "## 3. NHGIS Geography Shapefile Metadata Exploration\n",
    "\n",
    "Now that we have explored the metadata for the NHGIS time-series tables and made a selection, the next step is to explore the metadata for the NHGIS spatial files and make our selection for the geographic component of our extration.  Refer to the [list of GIS Files](https://www.nhgis.org/gis-files) on the IPUMS NHGIS website for information on the spatial data available from the NHGIS repository.\n",
    "\n",
    "GIS files in the NHGIS repository are stored as [shapefiles](https://en.wikipedia.org/wiki/Shapefile).  A shapefile is a commonly-used type of file format for geographic vector data.  Vector data is one of the main types of geographic data and can represent point locations, lines, or polygon (geographic boundary) data.  The GIS files in the NHGIS repository include geographic boundary data relevant to the Census and ACS including boundaries for states, counties, Census tracts, ZIP code areas, and many more.\n",
    "\n",
    "As we saw in our metadata exploration above, available geographies vary based on the dataset.  If we want to extract geographic data along with the our time-series data table, we will need to make sure we select an appropriate file for use with our data.\n",
    "\n",
    "### 3a. Review Time-Series Data Metadata\n",
    "\n",
    "First let's review which geographies are relevant for our the time-series table we selected and choose a gepgraphic level for our data extraction from this list.  As we saw in the time-series exploration steps, the available geographic levels vary based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac08e4-2ba7-44b2-adfb-4bf2652db967",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metadata_nhgis(time_series_table = selection_datts)$geog_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6aaae-bf3f-4b7b-8e8e-6cbc11a58080",
   "metadata": {},
   "source": [
    "For this example, let's choose the *county* geographic level for our extraction.  Let's save the code to the *selection_geog* object for use later on.\n",
    "\n",
    "**★ Pro Tip:** Similar to the dataset selection step in 2d, you can select multiple datasets by passing a list of geography codes (e.g. *c(\"state\", \"county\")*) in the extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_geog <- \"county\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658bf8f-4a2a-49a6-b143-b8dfdc65b62a",
   "metadata": {},
   "source": [
    "### 3b. Retrieve and Filter Shapefile Metadata by Criteria\n",
    "\n",
    "Next we will retrieve and filter the shapefile metadata and identify which shapefile we want to extract along with our time-series data.\n",
    "\n",
    "In this filtering step, we will also filter the available GIS files based on the year which will help us simplify the list of shapefiles to choose from.  Since we are using time-series table *CL8* for this exercise, which contains information on total population harmonized to 2010 geographies, we should will select shapefile which corresponds to 2010 geographies.\n",
    "\n",
    "**★ Pro Tip:** If you are unfamiliar with Census geographies, it might sound strange to include a year specification in your filtring step.  For large geographies, such as \"nation\" or \"state\", the year is relatively unimportant because the boundaries of these regions do not change over time.  However, for smaller geographies, especially those related to the U.S. Decennial Census, such as *tract*, *block* or *blck_grp* (block group), and as those related to political districts, such as *cd* (congressional district), the geographic boundaries can change over time.  Census tract, block group, and block boundaries are redrawn for each Decennial Census based on population numbers, and Congressional Districts are often redrawn for new congresional elections.  For this reason, it is essential to carefully choose your shapefiles to correspond with your time-series data extraction.\n",
    "\n",
    "The code below uses the [*get_metadata_nhgis*](https://rdrr.io/cran/ipumsr/man/get_metadata_nhgis.html) from the [**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html) package to retrive metadata on the NHGIS shapefiles and then filters the metadata to only the files for *2010* and at our previously-selected geographic level (county)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57327c9c-3f3f-434f-8093-2c8aabe6b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_shp <- get_metadata_nhgis(\"shapefiles\") %>%\n",
    "    filter(year == 2010 & grepl(selection_geog, geographic_level, ignore.case = T)) %>%\n",
    "    print(n = Inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce420a-65cc-413f-b8a5-6fd9a7934bf9",
   "metadata": {},
   "source": [
    "### 3d. Select a Geography Shapfile\n",
    "\n",
    "The filtering process has returned five relevant shapefiles which are all from 2010.  From this list we can see that two of the shapefiles are at the *county* geographic level, two are at the *county subdivision* geographic level, and one is at the *count (centers of popultion)* geographic level.  All five were returned because they all matched our filtering step which searched for files with the word \"county\" in the *geographic_level* metadata slot.\n",
    "\n",
    "Because we are specifically interested in *county* data, we will choose from the two *county* files:\n",
    "\n",
    "* *us_county_2010_tl2010* (2010 Tiger/Line files)\n",
    "* *us_county_2010_tl2020* ([2010 boundaries based on 2020 Tiger/Line files](https://www.nhgis.org/gis-files#2010-from-2020:~:text=coastal%20water%20areas.-,2010%20Boundaries%20Based%20on%202020%20TIGER/Line%20Files,-NHGIS%20also%20provides))\n",
    "\n",
    "**★ Pro Tip:** [TIGER/Line shapefiles](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html) are a collection of geographic datasets that contain information about the United States. They are derived from the [U.S. Census Bureau's](https://www.census.gov) Master Address File/Topologically Integrated Geographic Encoding and Referencing (MAF/TIGER) database.\n",
    "\n",
    "For this exercise we will use the **2010 Tiger/Line files** which is referred to using identification code **us_county_2010_tl2010**.  Again we will store the identification code so we can easily retrive it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cdadfb-24ad-4c55-a6c5-64f36c30e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_shp <- \"us_county_2010_tl2010\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ffa3a-9550-4ccd-bc2e-cdc53ccdcff7",
   "metadata": {},
   "source": [
    "## 4. NHGIS Time-Series Dataset and Geography Shapefile Extraction Specification and Submission\n",
    "\n",
    "Once we have reviewed the available data and decided on the time-series tables and GIS files we want, the next step is to set up a data extraction using the [*define_extract_nhgis*](https://rdrr.io/cran/ipumsr/man/define_extract_nhgis.html) function from the [**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html) package.  This function requires the following minimum parameters:\n",
    "\n",
    "* *description* · text description of the extract\n",
    "* *time_series_tables* · a  *tst_spec* object which specifies the parameters of a time-series table and requires the following minimum parameters:\n",
    "  * *name* · vector of time-series tables to include in the extract; tables should be specified using the sample identification codes\n",
    "  * *geog_levels* · vector of geographic levels to be included in the extract; geographic levels should be specified using the geographic level identification codes\n",
    "* *shapefiles* · vector of GIS shapefiles to include in the extract\n",
    "\n",
    "### 4a. Define the Data Extract\n",
    "\n",
    "We already know what we will pass to the function for the *name* (\"CL8\") and *geog_levels* (\"county\") of the *tst_spec* object passed to the *time_series_tables* parameter and for the *shapefiles* (\"us_tract_2010_tl2010\") parameter.  We are ready to submit our data extract request.  In this step we will add a text description of the request which can be anything and is included to help us differentiate between requests.  For this extract we will use the simple description \"IPUMS NHGIS Data Extraction\".\n",
    "\n",
    "Here we pass all the extraction definition information to the [*define_extract_nhgis*](https://rdrr.io/cran/ipumsr/man/define_extract_nhgis.html) function from the [**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html) package and store the resulting extraction definition in the object *extract_definition*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6fbfd-9e94-47c5-8fac-78dbb8983e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_definition <- define_extract_nhgis(description = \"I-GUIDE NHGIS Data Extraction\",\n",
    "                                           time_series_tables = tst_spec(name = selection_datts,\n",
    "                                                                         geog_levels = selection_geog),\n",
    "                                           shapefiles = selection_shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560de1c0-afbc-4a84-8260-9688df775926",
   "metadata": {},
   "source": [
    "Let's review the extraction definition information to make sure we have set it up the way we intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38928524-d025-4612-a2a1-91ef5860e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the extraction definition\n",
    "extract_definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee2332-0732-4fdb-a499-21d329d31c96",
   "metadata": {},
   "source": [
    "Everything looks good so we will submit the extraction request, wait for it to complete, and download the resulting data.\n",
    "\n",
    "### 4b. Submit the Extract Request\n",
    "\n",
    "Now that the extraction definition is set up, we can submit it to the IPUMS API using the [*submit_extract*](https://rdrr.io/cran/ipumsr/man/submit_extract.html) from the [**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html).\n",
    "\n",
    "For this exercise, after submitting the request we will also use the [*wait_for_extract*](https://rdrr.io/cran/ipumsr/man/wait_for_extract.html) function from the [**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html) package to monitor the status of the request.  This is not a necessary step but it is helpful, especially when submitting large requests.\n",
    "\n",
    "Finally, once the extract is complete, we can download it using the [*download_extract*](https://rdrr.io/cran/ipumsr/man/download_extract.html) function from the [**ipumsr**](https://cran.r-project.org/web/packages/ipumsr/index.html) package and save it in the object *filepath*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c955fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit extraction  \n",
    "extraction_submitted <- submit_extract(extract_definition)\n",
    "\n",
    "# wait for completion\n",
    "extraction_complete <- wait_for_extract(extraction_submitted)\n",
    "\n",
    "# check completion\n",
    "extraction_complete$status\n",
    "\n",
    "# get extraction filepath\n",
    "filepath <- download_extract(extraction_submitted, overwrite = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc34a6-2b19-4dba-bd01-40c70d9e276c",
   "metadata": {},
   "source": [
    "### 4c. Review the Extract\n",
    "\n",
    "Once we have downloaded the extract, we are ready to review it and transform it to a format we can easily use.  The NHGIS data extract download will contain the following two files.\n",
    "\n",
    "1. A data file (file extension .cdv) containing the time-series tabular data.\n",
    "2. A zipped GIS shapefile (file extension .zip) containing the geographic boundary data.\n",
    "\n",
    "Read the data file and shapefile into formats which we can work with in R.  The final *dat* object will contain the data from our extraction in a [*tibble*](https://tibble.tidyverse.org) format and the *shp* object will contain our geographic boundaries in [*simple features* (*sf*)](https://r-spatial.github.io/sf/articles/sf1.html) format, a commonly-used format for spatial data objects in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99208189-2bfd-4a72-acb9-e4176842631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see files in extract\n",
    "dat <- read_nhgis(filepath[1])\n",
    "shp <- read_ipums_sf(filepath[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a0d54-2219-49b5-94e1-57cfbe5ba9fe",
   "metadata": {},
   "source": [
    "We now have a useable version of our time-series table stored in *dat*.  Let's take a look at the number of observations and variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb08915-46f6-43c5-9faf-21dd553ba6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b489527-6d8d-4160-bd53-ff7006ce11f1",
   "metadata": {},
   "source": [
    "The data we downloaded includes information on 16 variables for 3143 counties.\n",
    "\n",
    "Let's take a look at the first few lines of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f09da-1cd0-43b9-9ab7-bf2801b446fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc170b5-fc6b-4051-959e-6c49de83d64a",
   "metadata": {},
   "source": [
    "We know that our data extraction includes total population information and these data are represented by the following variables:\n",
    "\n",
    "**Population Variables**\n",
    "* 1990 Total Population Estimate (CL8AA1990)\n",
    "* 1990 Total Population Lower Bound (CL8AA1990L)\n",
    "* 1990 Total Population Upper Bound (CL8AA1990U)\n",
    "* 2000 Total Population Estimate (CL8AA2000)\n",
    "* 2000 Total Population Lower Bound (CL8AA2000L)\n",
    "* 2000 Total Population Upper Bound (CL8AA2000U)\n",
    "* 2010 Total Population (CL8AA2010)\n",
    "* 2020 Total Population Estimate (CL8AA2020)\n",
    "* 2020 Total Population Lower Bound (CL8AA2020L)\n",
    "* 2020 Total Population Upper Bound (CL8AA2020U)\n",
    "\n",
    "In addition to the 1990, 2000, 2010, and 2020 total population values, the data also include upper and lower confidence limits for the 1990, 2000, and 2020 total population estimates.  If we remember from our data extraction specification steps, we chose a time-series table that was standardized to the 2010 geographies.  Because the 1990, 2000, and 2020 populations had to be estimated for slightly different geographies than the geographies for which the original data was collected, there is some potential error in the estimates.  The upper and lower confidence limits provide information on the possible error in the population estimates.\n",
    "\n",
    "In addition to the population variables, the downloaded data also includes multiple geographic variables.\n",
    "\n",
    "**Geographic Variables**\n",
    "* GIS Join Match Code (GISJOIN)\n",
    "* Geography Year (GEOGYEAR)\n",
    "* State Name (STATE)\n",
    "* State FIPS Code (STATEA)\n",
    "* County Name (COUNTY)\n",
    "* County FIPS Code (COUNTYA)\n",
    "\n",
    "Next let's take a look at the number of observations and variables in the geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c6af3-15db-40bf-b8c1-2ec9dd34a708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim(shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e034c542-9477-4cae-aab7-782324f46eb9",
   "metadata": {},
   "source": [
    "The geographic data includes information on 21 variables for 3221 counties.\n",
    "\n",
    "Let's take a look at columns included in the geographic data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5a8a4-0638-4ea6-a32b-1dde36843ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e32c5-cbfe-4810-9c10-71d9e80e527b",
   "metadata": {},
   "source": [
    "The variable we are most interested in from the geographic data is the GIS Join Match Code (GISJOIN).  This variable corresponds to the GISJOIN variable in our time-series tabular data and we will use the two corresponding variables from the two datasets to link them.\n",
    "\n",
    "### 4d. Merge the Time-Series and Geography Data\n",
    "\n",
    "Now that we have both the time-series data table on total population for 1990, 2000, 2010, and 2020 by county and the geographic data file with county boundaries, we can merge the two to create a spatially referenced data object which includes the total population counts by county.  We will be able to use this file to create map visualization of the population data or conduct other spatial analysis workflows.\n",
    "\n",
    "In the previous section we identified the GIS Join Match Code (GISJOIN) variable present in both our tabular and geographic data.  We will use this common variable as the join key and merge the two files and create a new version of our which includes both popuation and geographic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d7e5c-f3e1-464b-8893-e57ef219d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the time-series population data with the county geographic data\n",
    "dat_shp <- merge(dat, shp, by = \"GISJOIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca8c9f-2653-423d-b71b-24bbc8e12a0f",
   "metadata": {},
   "source": [
    "The final merged includes total population for 1990, 2000, 2010, and 2020 attached to 2010 county geographic boundaries.  The code below provides a snapshot of the first ten lines in the final merged dataset showing all variables from both tables including all population variables and the *geometry* column containing geographic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa05f2-af45-45ed-91e7-8ad2cc99467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(dat_shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e09de-21d3-4ea6-8f08-50e9d79e2cbe",
   "metadata": {},
   "source": [
    "### 4e. Save the Data\n",
    "\n",
    "Finally, let's save the data we extracted from IPUMS USA.  We will save the data as a **shapefile** (*.shp*).  The shapefile format will retain the geographic metadata necessary to use our file for mapping and spatial analysis.  This type of file can be reopened in R using the [*st_read*](https://rdrr.io/cran/sf/man/st_read.html) function from the [**sf**](https://cran.r-project.org/web/packages/sf/index.html) package or read natively into traditional GIS software platforms such as QGIS and ArcGIS.\n",
    "\n",
    "**★ Pro Tip:** Saving the geographic data as a shapefile will produce four files.  All four files together consititute the informaiton necessary to properly draw and spatially reference the data in the shapefile and should be kept together.\n",
    "\n",
    "* a *.shp* file containing feature geometry\n",
    "* a *.dbf* file containing feature attribute information\n",
    "* a *.shx* file containing the feature index\n",
    "* a *.prj* file containing projection information\n",
    "\n",
    "Because our data includes geographic data we can't save it to a tabular format such as a comma-separated values (.csv) file or the R Data Serilization (RDS) format as these formats do not support geographic metadata storage\n",
    "\n",
    "Since our data is very large let's first subset it to make it a little easier to work with.  Before saving, we will subset the data to include only individuals located in the state of California (FIPS code 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8321ac-b6c1-4077-89aa-bb96d01347d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the data to only the state of California\n",
    "dat_subset <- dat_shp[dat_shp$STATEA == \"06\",]\n",
    "\n",
    "# view the dimensions of the new data table\n",
    "dim(dat_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66272b04-ea9f-4822-aed0-9dd12c73655f",
   "metadata": {},
   "source": [
    "Subsetting the data to only California reduces the dimensions of the data to only 58 counties, making it much easier to work with and store.\n",
    "\n",
    "Before saving, let's also remove a few of the attributes which have very large values including land area (ALAND10), water area (AWATER10), and shape area (Shape_area).  Due to their large values these attibutes will cause warnings when we save our data to shapefile format so we will save ourself the headache of seeing all the warnings by removing the attributes prior to saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a480b-fffe-46cd-8670-f601af9faa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_subset <- dat_subset[, !names(dat_subset) %in% c(\"ALAND10\", \"AWATER10\", \"Shape_area\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f1901-51bc-4db2-9309-7b4672b80608",
   "metadata": {},
   "source": [
    "We are ready to save our data.  We wil use the [*st_write*](https://rdrr.io/cran/sf/man/st_write.html) function from the [**sf**](https://cran.r-project.org/web/packages/sf/index.html) package to write our data as a shapefile.\n",
    "\n",
    "**★ Pro Tip:** Setting the *delete_dsn* to *TRUE* for the [*st_write*](https://rdrr.io/cran/sf/man/st_write.html) function will allow the save to overwrite the file if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ad181-8451-4d0d-a6ce-882dca99738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_write(dat_subset, \"ipums_nhgis_example.shp\", driver = \"ESRI Shapefile\", delete_dsn = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c1143-3052-4786-9c77-066ff4ebdfc1",
   "metadata": {},
   "source": [
    "At the end of this exercise we have a freshly downloaded dataset from the IPUMS NHGIS repository saved in our workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313811a9-d540-4615-b871-ae4ebbee5cd4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "* Continue to [**Chapter 3.01.1 IPUMS NHGIS Data Extraction using ipumsr: Supplemental Exercise 1**](https://platform.i-guide.io/notebooks/a74fff96-4db5-430f-b346-958b0c5f7b38)\n",
    "* Continue to [**Chapter 3.01.2 IPUMS NHGIS Data Extraction using ipumsr: Supplemental Exercise 2**](https://platform.i-guide.io/notebooks/bc79eda6-8353-42ea-8cb7-5db70aa6febf)\n",
    "* Continue to [**Chapter 3.01.3 IPUMS NHGIS Data Extraction using ipumsr: Supplemental Exercise 3**](https://platform.i-guide.io/notebooks/55dd96e5-fdf6-408f-a050-7fcd006d0575)\n",
    "* Move on to Chapter 5: Data Cleaning, Preparation, and Exploratory Data Analysis (EDA)\n",
    "  * [**Chapter 5.02 Spatial Data Exploration and Preprocessing with IPUMS NHGIS**]()\n",
    "* Return to the [**R Spatial Notebooks Project Chapter List**](https://vavramusser.github.io/r-spatial/#:~:text=Chapter%201%3A%20Data%20Sources%20and%20APIs) to view a list of all available notebooks organized in the R Spatial Notebooks chapter structure.\n",
    "* Visit the [**R Spatial Notebooks Project Homepage**](https://vavramusser.github.io/r-spatial) to learn more about the project, view the list of all notebooks, and explore additional resources.\n",
    "* Join the project [**Mailing List**](https://mailchi.mp/ab01e8fc8397/r-spatial-email-signup) to hear about future notebook releases and other updates.\n",
    "* If you have an idea for a new notebook please submit your idea via the [**Suggestion Box**](https://us19.list-manage.com/survey?u=746bf8d366d6fbc99c699e714&id=54590a28ea&attribution=false).\n",
    "\n",
    "---\n",
    "\n",
    "## ★ Thank You ★\n",
    "\n",
    "Thank you so much for engaging with this notebook and supporting the project!  The R Spatial Notebooks Project is a labor of love so if you enjoy or benefit from these notebooks, please consider [**Donating to the Project**](https://buymeacoffee.com/vavramusser).  Your support allows me to continue producing notebooks and supporting the R Spatial Notebooks community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570aa6d-f6cd-436a-87d5-6788778bd601",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Code\n",
    "A clean and simple version of the code included in this notebook (excluding the metadata exploration steps).  **Don't forget to update the code with your IPUMS API key!**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e43e1487-d725-4ebf-a26e-d06e52a95ddc",
   "metadata": {},
   "source": [
    "# install required packages\n",
    "#install.packages(\"ipumsr\", \"sf\")\n",
    "\n",
    "# load necessary libraries\n",
    "library(ipumsr, sf)\n",
    "\n",
    "# set IPUMS API key\n",
    "ipums_api_key <- \"paste your api key here\"\n",
    "set_ipums_api_key(ipums_api_key, save = T, overwrite = T)\n",
    "\n",
    "# define extract specifications\n",
    "selection_datts <- \"CL8\"                  # time-series data table\n",
    "selection_geog <- \"tract\"                 # geographic level\n",
    "selection_year <- \"2010\"                  # year(s)\n",
    "selection_shp <- \"us_tract_2010_tl2010\"   # shapefile\n",
    "\n",
    "# set up the data extraction\n",
    "extraction <- define_extract_nhgis(description = \"IPUMS NHGIS Data Extraction\",\n",
    "                                   time_series_tables = tst_spec(name = selection_datts,\n",
    "                                                                 geog_levels = selection_geog,\n",
    "                                                                 years = selection_year),\n",
    "                                   shapefiles = selection_shp)\n",
    "\n",
    "# submit extract request and download the files\n",
    "extraction_submitted <- submit_extract(extraction)                  # submit the extract  \n",
    "extraction_complete <- wait_for_extract(extraction_submitted)       # wait for completion\n",
    "extraction_complete$status                                          # check completion\n",
    "filepath <- download_extract(extraction_submitted, overwrite = T)   # get extract filepath\n",
    "\n",
    "# extract the files\n",
    "dat <- read_nhgis(filepath[1])\n",
    "shp <- read_ipums_sf(filepath[2])\n",
    "\n",
    "# merge the data and geography files\n",
    "dat_shp <- merge(dat, shp, by = \"GISJOIN\")\n",
    "\n",
    "# save as a Shapefile\n",
    "st_write(dat_shp, \"ipums_nhgis_example.shp\", driver = \"ESRI Shapefile\", delete_dsn = T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
